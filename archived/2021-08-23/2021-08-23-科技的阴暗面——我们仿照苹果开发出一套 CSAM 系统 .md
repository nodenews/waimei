---
layout: post
title: "科技的阴暗面——我们仿照苹果开发出一套 CSAM 系统 "
date: 2021-08-23T11:05:21.000Z
author: Solidot
from: https://www.solidot.org/story?sid=68648
tags: [ Solidot ]
categories: [ Solidot ]
---
<!--1629716721000-->
[科技的阴暗面——我们仿照苹果开发出一套 CSAM 系统](https://www.solidot.org/story?sid=68648)
------

<div>
本月初，苹果公司推出一套系统，专门用于扫描 iPhone 与 iPad 照片中的儿童性虐待素材（CSAM）。这一消息引发一场民权风暴，就连苹果自己的员工也表达了担忧。但该公司坚称，针对该系统的争议完全属于“误解”。真的吗？我们不信。为此，我们<a href="https://www.washingtonpost.com/opinions/2021/08/13/apple-csam-child-safety-tool-hashing-privacy/?itid=lk_inline_manual_5" target="_blank">撰写</a>了一篇仅供同行评审的论文，探讨为什么开发这样一套系统具有极高的危险。如果真是误解就好了，至少能证明我们是错的。但问题是，我们很清楚它的工作原理，也知道它真的很危险。我们的<a href="https://www.usenix.org/conference/usenixsecurity21/presentation/kulshrestha" target="_blank">研究项目</a>始于两年之前，希望通过一套实验系统识别端到端加密在线服务中的儿童性虐待内容。作为安全研究人员，我们都清楚端到端加密本身的价值，特别是在保护数据免受第三方访问中的意义。但我们也对加密平台上儿童性虐内容的激增感到震惊，也担心在线服务坚持维持现状、不愿在打击这类非法内容上付出努力。我们希望探索一种可能的中间立场，即由在线服务识别出有害内容，同时继续保持良好的端到端加密效果。其中的概念非常简单：如果有人分享了与已知有害内容数据库相匹配的素材，则服务会自动发出警报。如果有人分享了正常内容，则服务照常放行。人们无法读取数据库内容、也不清楚怎样的内容才算匹配，因为一旦公开，犯罪分子就有可能想办法逃避检测。但我们很快遇上了明显的问题。我们的系统很容易在调整后被用于其他监控与审查，而且不受限于特定类别的内容。换句话说，只要更换其他内容匹配数据库，任何人都能把它迅速转化为打击舆论、排除异己的工具。
</div>
