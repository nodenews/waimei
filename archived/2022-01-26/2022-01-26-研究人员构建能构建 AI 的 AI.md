---
layout: post
title: "研究人员构建能构建 AI 的 AI"
date: 2022-01-26T12:29:52.000Z
author: Solidot
from: https://www.solidot.org/story?sid=70519
tags: [ Solidot ]
categories: [ Solidot ]
---
<!--1643200192000-->
[研究人员构建能构建 AI 的 AI](https://www.solidot.org/story?sid=70519)
------

<div>
人工智能很大程度上是数字游戏。10 年前深度神经网络开始超越传统算法，这是因为我们终于拥有了足够的数据和处理能力。深度神经网络是一种学习识别数据模式的人工智能形式。今天的神经网络愈加渴望数据和算力。网络的参数有数百万甚至是数十亿个，参数代表了人工神经元之间连接的强度，训练神经网络需要对其仔细调整。目标是寻找到近于理想的值，该过程被称为优化，但是训练网络达到这一点并不容易。伦敦 DeepMind 的研究科学家 <a href="https://petar-v.com/" target="_blank">Petar Veličković</a> 表示：“训练可能需要几天、几周甚至是几个月的时间。”<br><br>这种情况也许很快会改变。安大略省圭尔夫大学的 <a href="https://bknyaz.github.io/" target="_blank">Boris Knyazev</a> 和同事<a href="https://www.quantamagazine.org/researchers-build-ai-that-builds-ai-20220125/" target="_blank">设计并训练了一个“超网络(hypernetwork)”</a>——一种其他神经网络的霸主——可以加快训练的过程。对于一个为了某项任务设计的、未经训练的新神经网络，超网络可以在几分之一秒内预测出它的参数，从理论上让训练变得不再有必要。由于超网络学习了深度神经网络设计中极其复杂的模式，因此这项工作也可能具有更深层次的理论意义。<br><br>现阶段超网络在某些环境中的表现好得出人意料，但仍然有改善的空间——考虑到问题的量级，这是很自然的。Veličković 表示，如果他们能解决这个问题，“将对机器学习产生全方位的影响。”<br><br>目前训练和优化神经网络的最好方法是随机梯度下降法（SGD）技术的变体。训练涉及最小化网络在给定任务（例如图像识别）中所犯的错误。SGD 算法通过大量标记数据调整网络参数并减少错误或损失。梯度下降是从损失函数的高值下降到某个最小值的迭代过程，代表了足够好的（有时甚至是最好的）参数值。<br><br>但是这种技术只有在你有一个网络需要优化的时候才有效。为了构建最初的神经网络——通常由从输入到输出之间的多层人工神经元组成，工程师必须依靠直觉和经验法则。这些架构在神经元的层数，每层的神经元数量等方面会有所不同。
</div>
