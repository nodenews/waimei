---
layout: post
title: "计算机科学家证明为什么更大的神经网络表现更好"
date: 2022-02-11T09:35:27.000Z
author: Solidot
from: https://www.solidot.org/story?sid=70629
tags: [ Solidot ]
comments: True
categories: [ Solidot ]
---
<!--1644572127000-->
[计算机科学家证明为什么更大的神经网络表现更好](https://www.solidot.org/story?sid=70629)
------

<div>
人类多亏有对生拇指。但就算演化给了我们更多的拇指，情况也不会有太大的改善。每只手有一个拇指就足够了。神经网络不是这样，神经网络是执行类人任务的先进人工智能系统。随着它们变得更大，它们就能掌握更多。这会让旁观者大吃一惊。基本的数学结果表明，网络应该只需要这么大，但是现代神经网络的规模通常会远超出预测的需求——这种情况被称为过度参数化。在 12 月会议 NeurIPS 上发布的<a href="https://arxiv.org/abs/2105.12806" target="_blank">一篇论文</a>中，微软研究院的 Sébastien Bubeck 和斯坦福大学的 Mark Sellke 为规模放大成功背后的奥秘<a href="https://www.quantamagazine.org/computer-scientists-prove-why-bigger-neural-networks-do-better-20220210/">提出了一种新的解释</a>。他们表明，神经网络必须比传统预期的大得多，才能避免某些基本问题。这一发现为一个持续了几十年的问题提供了一般性的见解。对神经网络规模的标准预期来自对它们如何记忆数据的分析。但要了解记忆，我们必须首先了解网络的作用。神经网络的一项常见任务是识别图像中的对象。研究人员首先为其提供许多图像和对象标签，训练它学习两者之间的相关性。之后网络将正确识别它看过的图像中的对象。换句话说，训练使网络记住数据。更值得注意的是，一旦网络记住了足够多的训练数据，它就能以不同程度的准确度预测它从未见过的物体的标签。后一个过程被称为泛化。
</div>
